{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data - Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When we talk about cleaning data, there are some steps (they are iterative, and not linear) that can be taken in order to have tidy data.\n",
    "\n",
    "### 1. Data Quality\n",
    "### 2. Inspection\n",
    "### 3. Actual Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality can be broken down into:\n",
    "\n",
    "### 1.1. Validity\n",
    "#### The degree to which data conform to defined business rules or constraints.\n",
    "#### - Making sure that values in a particular column are of a particular datatype, e.g., boolean, numeric, date.\n",
    "#### - Certain columns cannot be empty\n",
    "#### - Numbers or dates should fall within a certain range\n",
    "#### - Making sure text fields are in a certain pattern, e.g., phone numbers like (999) 999-9999\n",
    "#### - Making sure certain conditions in multiple fields hold. For instance, a patient's date of discharge from the hospital cannot be earlier than the date of admission.\n",
    "### 1.2. Accuracy\n",
    "#### The degree to which data is close to the true values.\n",
    "#### A valid street address, a valid eye color, a valid number for a specific range.\n",
    "### 1.3. Completeness\n",
    "#### The degree to which all required data is known.\n",
    "#### Missing data is gonna happen anyways, so maybe the solution is to get more data.\n",
    "### 1.4. Consistency\n",
    "#### The degree to which data is consistent, within the same dataset or across multiple datasets.\n",
    "#### It's to make sure values don't contradict themselves, or that they make sense. For example, a person age 10 can't be married or divorced.\n",
    "### 1.5. Uniformity\n",
    "#### The degree to which data is specified using the same unit of measure\n",
    "#### Weight, for instance, may be recorded either in pounds or kilos. Date should follow USA format or European format.\n",
    "#### Data must be converted to a single measure unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's when we inspect and explore the underlying data for error detection. It can also be broken donw into:\n",
    "\n",
    "### 2.1. Data Profiling\n",
    "#### It is a summary statistics about the data that can give a general idea about the quality of the data\n",
    "#### - Is a data column recorded as string or number?\n",
    "#### - How many values are missing?\n",
    "#### - How many unique values in a column and their distribution?\n",
    "#### - Does this dataset has a relationship with another dataset, or is it linked somehow?\n",
    "### 2.2. Visualizations\n",
    "#### By analysing and visualizing the data using statistical methods such as mean, standard deviation, range, or quantiles, one can find values that are unexpected and thus erroneous.\n",
    "#### Some examples may be outliers, people in a country, for instance, who earn much more thatn anyone else. Outliers are not necessarely incorrect data. They must be further investigated.\n",
    "### 2.3. Software packages\n",
    "#### There are many software packages or libraries that can be used to make sure the data is not violating those constraints.\n",
    "#### In Python we can use:\n",
    "#### - Pandas\n",
    "#### - Numpy\n",
    "#### - Seaborn\n",
    "#### - Matplotlib and Matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Actual Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning involve different techniques, and different methods can be applied depending on the problem and datatype. Overall, incorrect data is either removed, corrected, or imputed.\n",
    "\n",
    "### 3.1. Irrelevant data\n",
    "#### Data that are not actually needed and don't fit under the context of the problem we are trying to solve.\n",
    "#### For example, if we are analysing data about the general health of the population, the phone number wouldn't be necessary (column-wise).\n",
    "#### If we are interested in only a particular country, we don't want to include all the other countries (row-wise).\n",
    "#### However, we have to BE SURE that the data is really unimportant. Even if there is no correlation, we should always ask a domain expert about the importance of a particular data field or column.\n",
    "### 3.2. Duplicates\n",
    "#### Data points that are repeated in the dataset. They should simply be removed.\n",
    "### 3.3. Type conversion\n",
    "#### Making sure numbers are stored as numerical data types, date as date object, and so on.\n",
    "#### Categorical variables should be converted into and from numbers if needed.\n",
    "### 3.4. Syntax Errors\n",
    "#### Remove white spaces\n",
    "#### Pad strings: for example, some numerical codes are often represented with prepending zeros to ensure they always have the same number of digits, e.g. 313 -> 000313 (6 digits)\n",
    "#### Fix Typos: strings can be entered in different ways (female, fem., Female, FemalE...). One solution is to map each value: dataframe['gender'].map({'m': 'male', 'fem.': 'female', 'Female': 'female', ...}).\n",
    "#### Standardize: Making sure strings are all upper or lower case, making sure all values have a certain measurement unit, dates are in the same format, etc.\n",
    "#### Scaling/Transformation: Scaling means to transform the data so that it fits within a specifc scale, such as 0-100 or 0-1.\n",
    "#### Normalization: While normalization also rescales the values into a range of 0-1, the intention here is to transform the data so that it is normally distributed. In most cases, we normalize the data if we are going to be using statistical methods that rely on normally distributed data.\n",
    "#### Missing values: Drop the rows containing them, if they rarely happen and occur at random. Imput them using statistical methods like mean or median, or using linear regression or knn imputation. Flag them, since, sometimes, missing data is informative in itself and everytime we are droping or imputing data we are losing information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Code for Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: If we get an 'encoding' error while loading the dataset, we can use the encoding parameter:\n",
    "\n",
    "#### df1 = pd.read_csv('unclean_data.csv', encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename(columns = {'DURATION': 'TIME'})  -> the DURATION column will be renamed as 'TIME'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first thing to do is to have a general idea of the dataset you want to clean.\n",
    "### In order to do that, and considering a dataframe called 'df', we can use the following code:\n",
    "\n",
    "### df.head() -> to see the first 5 rows\n",
    "### df.tail() -> to see the last 5 rows\n",
    "### df.shape -> to see the number of rows and columns\n",
    "### df.columns -> to see the name of all columns\n",
    "### df.info() -> to have a general idea of the dataset\n",
    "### df.describe() -> to see the statistics (min, max, quantiles, etc)\n",
    "### df.dtypes() -> to see the data types of each column\n",
    "### df['some_column_name'].value_counts(dropna = False) -> to analyse a specific column (great for categorical data) and\n",
    "### see the number of distinct observations. The dropna argument is to be able to count missing data as well\n",
    "### df['some_column_name'].describe() -> to see statistics of a specific column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It can also be useful to use plots in order to have an idea of the data, especially considering outliers.\n",
    "## We can use barplots, histograms, boxplots, scatterplots\n",
    "\n",
    "### Bar plots: for discrete (can be counted) data counts\n",
    "### Histograms: for continuous (can't be counted) data counts\n",
    "#### df.population.plot('hist')\n",
    "#### plt.show()\n",
    "### Box Plots: to visualize basic summary statistics (outliers, min, max, percentiles, etc.)\n",
    "#### df.boxplot(column = 'population', by = 'continent')\n",
    "#### plt.show()\n",
    "### Scatter Plots: to see relationship between 2 numeric variables\n",
    "#### df.plot(kind = 'scatter', x = 'initial_cost', y = 'total_est_fee', rot=70)\n",
    "#### plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful code for tidying and Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sometimes we have columns containing values instead of variables, and sometimes the other way around.\n",
    "### We can turn rowns into columns (pd.melt) and vice-versa (pivot() or pivot_table()) by using the following code:\n",
    "\n",
    "### pd.melt(frame = df, id_vars = 'name', value_vars = ['treatment a', 'treatment b'], var_name = 'treatment', value_name = 'result')\n",
    "### frame is the dataset, id_vars is the column to be fixed, value_vars is to specify which columns to melt, var_name and value_name is to rename the melted columns\n",
    "### weather_tidy = weather.pivot(index = 'date', columns = 'element', values = 'value')\n",
    "### weather_tidy = weather.pivot(index = 'date', columns = 'element', values = 'value', aggfunc = np.mean)\n",
    "\n",
    "## We can also melt and parse, to create new columns\n",
    "\n",
    "### df_melt['sex'] = df_melt.variable.str[0] -> here we are creating a new column called 'sex' that uses jus the first letter of the observations in column 'variable'\n",
    "### ebola_melt['str_split] = ebola_melt.type_country.str.split('_') -> here we are creating a new column called 'str_split' which will get every observation from the column 'type_country' and split it on the '_' , thus creating a list. The 'str' is to make it a string.\n",
    "### ebola_melt['type'] = ebola_melt.str_split.str.get(0) -> here we are creating a new column called 'type' where we get the first item from the previously created column, 'str_aplit'.\n",
    "\n",
    "## To combine different datasets we can use the concat() function\n",
    "\n",
    "### It can be done row-wise (when the datasets are similar):\n",
    "#### row_concat = pd.concat([uber1, uber2, uber3])\n",
    "### Or it can be done column-wise (when the datasets have different columns):\n",
    "#### ebola_tidy = pd.concat([ebola_melt, status_country], axis=1)\n",
    "\n",
    "## If you have many files that can be a pain to concatenate them one by one, you can use the glob module and a for loop to do that.\n",
    "\n",
    "### import glob\n",
    "### pattern = '*.csv'\n",
    "### csv_files = glob.glob(pattern)\n",
    "\n",
    "## If you want to read just one of the files, the first one for instance:\n",
    "### csv2 = pd.read_csv(csv_files[0]) \n",
    "\n",
    "## To read many files:\n",
    "### frames = []\n",
    "### for csv in csv_files:\n",
    "### -df = pd.read_csv(csv)\n",
    "### -frames.append(df)\n",
    "### uber = pd.concat(frames)\n",
    "\n",
    "## We can also merge data (like in SQL):\n",
    "### o2o = pd.merge(left = site, right = visited, left_on = 'name', right_on = 'people')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Cleaning Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We might want to convert columns from numeric to string, or from string to numeric or categorical. That's how we do it:\n",
    "\n",
    "### df['treatment b'] = df['treatment b'].astype(str) -> converting to string\n",
    "### df['sex'] = df['sex'].astype('category') -> converting to categorical \n",
    "### df['treatment a'] = pd.to_numeric(df['treatment a'], errors = 'coerce') -> converting from string to numeric. The argument errors = 'coerce' is gonna change invalid entries into missing values.\n",
    "\n",
    "## In order to clean strings, we can use regular expressions\n",
    "### First, we import the module:\n",
    "#### import re\n",
    "### Second, we compile the pattern:\n",
    "#### prog = re.compile('\\d{3}-\\d{3}-\\d{4}') -> this is a pattern of a phone number (xxx-xxx-xxxx)\n",
    "### Third, we test if the pattern works by using the .match() method on prog\n",
    "#### result = prog.match('123-456-7890')\n",
    "#### print(bool(result))\n",
    "\n",
    "## There is a way to extract numbers from a string to use them later:\n",
    "### It's the findall() function. The first argument is the pattern, and the second is the string\n",
    "#### matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')\n",
    "#### The + sign ensures that 10 is viewed as one number, and not as 1 and 0\n",
    "\n",
    "## Here goes some examples of pattern matching\n",
    "#### pattern1 = re.match(pattern = '\\d{3}-\\d{3}-\\d{4}', string = '123-456-7890')\n",
    "#### pattern3 = re.match(pattern = '[A-Z]\\w*', string = 'Australia') -> the \\w* is to match an arbitrary number of alphanumeric characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern2 = re.match(pattern = '\\$\\d*\\.\\d{2}', string = '$123.45')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting custom functions to clean data\n",
    "\n",
    "## The below function is to get the gender column and create a new column assigning 0 to female, 1 to male, and null to an observation that is not female or male\n",
    "### def record_gender(gender):\n",
    "### if gender == 'Female':\n",
    "### return 0\n",
    "### elif gender == 'Male':\n",
    "### return 1\n",
    "### else:\n",
    "### return np.nan\n",
    "\n",
    "## After that we can apply the function to a new column:\n",
    "### tips['recode'] = tips.sex.apply(recode_gender)\n",
    "\n",
    "## We can also use lambda functions to do that (2 different ways of doing):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$' , ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x)[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().sum()  -> this will give us a summary table with the count of missing values per column.\n",
    "# df.isnull().sum().sum()  -> this will give us the total number of missing values in the dataset\n",
    "# df.isnull().any()  -> this will give us a summary table of the columns that have missing data or not\n",
    "# ?df.dropna  -> will show a help window for the arguments of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To deal with missing data we can:\n",
    "\n",
    "### Remove duplicates\n",
    "#### tracks_no_duplicates = tracks.drop_duplicates()\n",
    "\n",
    "### Fill the NAs with the mean:\n",
    "#### oz_mean = airquality['Ozone'].mean()\n",
    "#### airquality['Ozone'] = airquality['Ozone'].fillna(oz_mean)\n",
    "\n",
    "## We can use assert statements to verify if the dataset has missing data or not. After the assert statement, if the dataset does not contains NAs, it should return nothing. If it has NAs it shoul return an error message.\n",
    "\n",
    "### assert pd.notnull(ebola).all().all()\n",
    "### For the above example, the dataset is called 'ebola', the first .all() will return a True or False for each column,  while the second .all() will return a single True or False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Missing Data Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Workflow for treating missing values is:\n",
    "### 1. Convert all missing values to null values\n",
    "### 2. Analyze the amount and type of missingness in the data\n",
    "### 3. Appropriately delete or impute missing values\n",
    "### 4. Evaluate and Compare the performance of the treated/imputed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data are usually filled with values like 'NA' , '-' , '.'\n",
    "### One way to see it is to use the .info() method\n",
    "### We can also store the unique values and sort them:\n",
    "#### csat_unique = college.csat.unique() -> college is the dataset and csat is one of its columns\n",
    "#### np.sort(csat_unique)\n",
    "\n",
    "## We can make those values null when we are loading the dataset:\n",
    "#### college = pd.read_csv('college.csv', na_values = '.')\n",
    "\n",
    "## We can use the .describe() function to see the dataset statistics. If we notice there is a column that look weird (the minimum or maximum values don't make sense) we can do the following:\n",
    "#### diabetes.BMI[diabetes.BMI == 0]  -> here we are selecting all the values that are zero, from the column BMI and dataset college.\n",
    "#### diabetes.BMI[diabetes.BMI == 0] = np.nan  -> here we are making those values null\n",
    "\n",
    "## A 'null' dataset can also be created where True implies missing data and False implies not missing. After that we can count the missing data.\n",
    "#### airquality_nullity = airquality.isnull()\n",
    "#### airquality_nullity.sum()\n",
    "\n",
    "## To get the percentage of missingness:\n",
    "#### airquality_nullity.mean() * 100\n",
    "\n",
    "## It's also possible to visualize missingness graphically\n",
    "## for that we use the missingno package\n",
    "#### import missingno as msno\n",
    "#### msno.bar(airquality)  -> to see a bar graph showing the amount in each column\n",
    "#### msno.matrix(airquality)  -> to see a nullity matrix, showing blank where there are missing values. The spark line on the right summarizes the general shape of the data with the number of columns in the bottom\n",
    "## In time-series data we use an additional parameter called frequency\n",
    "#### msno.matrix(airquality, freq = 'M')  -> M stands for month\n",
    "#### msno.matrix(airquality.loc['May-1976': 'Jul-1976'], freq = 'M')  -> it's possible to select a specific period of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data can be classified in 3 different groups:\n",
    "\n",
    "### Missing Completely at Random (MCAR)\n",
    "#### Missingness has no relationship between any value, observed or missing\n",
    "### Missing at Random (MAR)\n",
    "#### There is a systematic relationship between missingnessand other observed data, but not with the missing data\n",
    "### Missing not at random (MNAR)\n",
    "#### There is a relationship between missingness and its values, missing or non-missing\n",
    "\n",
    "## To find correlation between missingness we can use:\n",
    "### Heatmap - graph of correlation of missing values between columns\n",
    "#### msno.heatmap(diabetes)\n",
    "### Missingness Dendrogram - Describes correlations of variables by grouping them\n",
    "#### msno.dendogram(diabetes)\n",
    "\n",
    "## A great thing that can be done is to create a function that automates creating dummy values for missing data\n",
    "### We do this because the matplotlib module skips missing values when plotting, so we replace the missing values for dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import rand\n",
    "\n",
    "def fill_dummy_values(df, scaling_factor):\n",
    "    # Create a copy of the dataframe\n",
    "    df_dummy = df.copy(deep=True)\n",
    "    \n",
    "    # Iterate over each column\n",
    "    for col in df_dummy:\n",
    "        \n",
    "        # Get column, column missing values, and range\n",
    "        col = df_dummy[col]\n",
    "        col_null = col.isnull()\n",
    "        num_nulls = col_null.sum()\n",
    "        col_range = col.max() - col.min()\n",
    "        \n",
    "        # Shift and scale dummy values\n",
    "        dummy_values = (rand(num_nulls) - 2)\n",
    "        dummy_values = dummy_values * scaling_factor * col_range + col.min()\n",
    "        \n",
    "        # Return dummy values\n",
    "        col[col_null] = dummy_values\n",
    "    return df_dummy\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we are applying the function in order to visualize how missingness of a variable changes against another variable:\n",
    "\n",
    "### diabetes_dummy = fill_dummy_values(diabetes)  -> creating the dummy dataframe\n",
    "### nullity = diabetes.Serum_Insulin.isnull() + diabetes.BMI.isnull()  -> getting missing values of both columns for coloring\n",
    "### diabetes_dummy.plot(x = 'Serum_Insulin', y = 'BMI', kind = 'scatter', alpha = 0.5, c = nullity, cmap = 'rainbow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When and how to delete missing data\n",
    "\n",
    "### When the values are missing completely at random, we delete the whole row\n",
    "### msno.matrix(diabetes)  -> to check the missingness matrix and see the missingness correlation\n",
    "### diabetes['Glucose'].isnull().sum()  -> to count the number of missing values\n",
    "### diabetes.dropna(subset = ['Glucose'], how = 'any', inplace = True) -> to drop rows based on the missing values in the Glucose column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation\n",
    "\n",
    "### Basic Imputation Techniques:\n",
    "#### constant (e.g. 0)\n",
    "#### mean\n",
    "#### median\n",
    "#### mode (most frequent value)\n",
    "\n",
    "## Packages, code and process\n",
    "\n",
    "### Mean Imputation\n",
    "#### from sklearn.impute import SimpleImputer\n",
    "#### diabetes_mean = diabetes.copy(deep=True)  -> creating a copy of the dataset to compare later\n",
    "#### mean_imputer = SimpleImputer(strategy = 'mean')\n",
    "#### diabetes_mean.iloc[:, :] = mean_imputer.fit_transform(diabetes_mean)\n",
    "\n",
    "### Median Imputation\n",
    "#### from sklearn.impute import SimpleImputer\n",
    "#### diabetes_median = diabetes.copy(deep=True)  -> creating a copy of the dataset to compare later\n",
    "#### median_imputer = SimpleImputer(strategy = 'median')\n",
    "#### diabetes_median.iloc[:, :] = median_imputer.fit_transform(diabetes_median)\n",
    "\n",
    "### Mode Imputation\n",
    "#### from sklearn.impute import SimpleImputer\n",
    "#### diabetes_mode = diabetes.copy(deep=True)  -> creating a copy of the dataset to compare later\n",
    "#### mode_imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "#### diabetes_mode.iloc[:, :] = mode_imputer.fit_transform(diabetes_mode)\n",
    "\n",
    "### Constant Imputation\n",
    "#### from sklearn.impute import SimpleImputer\n",
    "#### diabetes_constant = diabetes.copy(deep=True)  -> creating a copy of the dataset to compare later\n",
    "#### constant_imputer = SimpleImputer(strategy = 'constant', fill_value = 0)\n",
    "#### diabetes_constant.iloc[:, :] = constant_imputer.fit_transform(diabetes_constant)\n",
    "\n",
    "## To Visualize imputations\n",
    "\n",
    "#### fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (10,10))\n",
    "#### nullity = diabetes['Serum_Insulin'].isnull() + diabetes['Glucose'].isnull()\n",
    "#### imputations = {'Mean Imputation': diabetes_mean, 'Median Imputation': diabetes_median, 'Most Frequent Imputation': diabetes_mode, 'Constant Imputation': diabetes_constant}\n",
    "#### for ax, df_key in zip(axes.flatten(), imputations):\n",
    "#### imputations[df_key].plot(x = 'Serum_Insulin', y = 'Glucose', kind = 'scatter', alpha = 0.5, c = nullity, cmap = 'rainbow', ax = ax, colorbar = False, title = df_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the imputations do not adjust to the shape of the data (if they are a straight line, for instance), it means that the imputations will bias the analysis. Therefore, we should use more dynamic imputations that will adjust better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Data\n",
    "\n",
    "### The first thing is to see how many NAs there are in the dataset\n",
    "#### airquality.isnull().sum()  -> the count of NAs in the dataset\n",
    "#### airquality.isnull().mean() * 100  -> the percentage of NAs per column\n",
    "\n",
    "### Some methods to fill NAs with other values:\n",
    "#### airquality.fillna(method = 'ffill', inplace = True)  -> replace NANs with last observed value\n",
    "#### airquality.fillna(method = 'bfill', inplace = True)  -> replace NANs with next observed value\n",
    "#### airquality.interpolate(method = 'linear', inplace = True)  -> impute linearly or with equidistant values\n",
    "#### airquality.interpolate(method = 'quadratic', inplace = True)  -> impute the values quadratically\n",
    "#### airquality.interpolate(method = 'nearest', inplace = True)  -> impute with the nearest observable value\n",
    "\n",
    "### We can visualize those imputes in a plot\n",
    "#### airquality['Ozone'].plot(title = 'Ozone', marker = 'o', figsize = (30,5))\n",
    "#### ffill_imputed = airquality.fillna(method = 'ffill')\n",
    "#### ffill_imputed['Ozone'].plot(color = 'red', marker = 'o', linestyle = 'dotted', figsize = (30,5))  -> this is to complete the first plot with the NANs imputation using the ffill method. We can do the same thing for the bfill method.\n",
    "\n",
    "### We can do the same for interpolations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set nrows to 3 and ncols to 1\n",
    "fig, axes = plt.subplots(3, 1, figsize=(30, 20))\n",
    "\n",
    "# Create a dictionary of interpolations\n",
    "interpolations = {'Linear Interpolation': linear, 'Quadratic Interpolation': quadratic, \n",
    "                  'Nearest Interpolation': nearest}\n",
    "\n",
    "# Loop over axes and interpolations\n",
    "for ax, df_key in zip(axes, interpolations):\n",
    "  # Select and also set the title for a DataFrame\n",
    "  interpolations[df_key].Ozone.plot(color='red', marker='o', \n",
    "                                 linestyle='dotted', ax=ax)\n",
    "  airquality.Ozone.plot(title=df_key + ' - Ozone', marker='o', ax=ax)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing using Fancy imputes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The KNN and Mice imputes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNN from fancyimpute\n",
    "from fancyimpute import KNN\n",
    "\n",
    "# Copy diabetes to diabetes_knn_imputed\n",
    "diabetes_knn_imputed = diabetes.copy(deep=True)\n",
    "\n",
    "# Initialize KNN\n",
    "knn_imputer = KNN()\n",
    "\n",
    "# Impute using fit_tranform on diabetes_knn_imputed\n",
    "diabetes_knn_imputed.iloc[:, :] = knn_imputer.fit_transform(diabetes_knn_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import IterativeImputer from fancyimpute\n",
    "from fancyimpute import IterativeImputer\n",
    "\n",
    "# Copy diabetes to diabetes_mice_imputed\n",
    "diabetes_mice_imputed = diabetes.copy(deep=True)\n",
    "\n",
    "# Initialize IterativeImputer\n",
    "mice_imputer = IterativeImputer()\n",
    "\n",
    "# Impute using fit_tranform on diabetes\n",
    "diabetes_mice_imputed.iloc[:, :] = mice_imputer.fit_transform(diabetes_mice_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Categorical values\n",
    "\n",
    "### Consists of:\n",
    "### 1. Convert non-missing categorical columnsto ordinal values\n",
    "### 2. Impute the missing values in the ordinal dataframe\n",
    "### 3. Convert back from ordinal values to ategorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding of a categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ordinal encoder\n",
    "ambience_ord_enc = OrdinalEncoder()\n",
    "\n",
    "# Select non-null values of ambience column in users\n",
    "ambience = users['ambience']\n",
    "ambience_not_null = ambience[ambience.notnull()]\n",
    "\n",
    "# Reshape ambience_not_null to shape (-1, 1)\n",
    "reshaped_vals = ambience_not_null.values.reshape(-1, 1)\n",
    "\n",
    "# Ordinally encode reshaped_vals\n",
    "encoded_vals = ambience_ord_enc.fit_transform(reshaped_vals)\n",
    "\n",
    "# Assign back encoded values to non-null values of ambience in users\n",
    "users.loc[ambience.notnull(), 'ambience'] = np.squeeze(encoded_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary ordinal_enc_dict\n",
    "ordinal_enc_dict = {}\n",
    "\n",
    "for col_name in users:\n",
    "    # Create Ordinal encoder for col\n",
    "    ordinal_enc_dict[col_name] = OrdinalEncoder()\n",
    "    col = users[col_name]\n",
    "    \n",
    "    # Select non-null values of col\n",
    "    col_not_null = col[col.notnull()]\n",
    "    reshaped_vals = col_not_null.values.reshape(-1, 1)\n",
    "    encoded_vals = ordinal_enc_dict[col_name].fit_transform(reshaped_vals)\n",
    "    \n",
    "    # Store the values to non-null values of the column in users\n",
    "    users.loc[col.notnull(), col_name] = np.squeeze(encoded_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Imputation of categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN imputer\n",
    "KNN_imputer = KNN()\n",
    "\n",
    "# Impute and round the users DataFrame\n",
    "users.iloc[:, :] = np.round(KNN_imputer.fit_transform(users))\n",
    "\n",
    "# Loop over the column names in users\n",
    "for col_name in users:\n",
    "    \n",
    "    # Reshape the data\n",
    "    reshaped = users[col_name].values.reshape(-1, 1)\n",
    "    \n",
    "    # Perform inverse transform of the ordinally encoded columns\n",
    "    users[col_name] = ordinal_enc_dict[col_name].inverse_transform(reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputations are used to improve model performance and low bias in the data\n",
    "### Imputation with maximum machine learning model performance is selected\n",
    "### The density plots explain the distribution in the data. The goal is to see which model most resembles the shape of the original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Summary of linar model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant to X and set X & y values to fit linear model\n",
    "X = sm.add_constant(diabetes_cc.iloc[:, :-1])\n",
    "y = diabetes_cc['Class']\n",
    "lm = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print summary of lm\n",
    "print('\\nSummary: ', lm.summary())\n",
    "\n",
    "# Print R squared score of lm\n",
    "print('\\nAdjusted R-squared score: ', lm.rsquared_adj)\n",
    "\n",
    "# Print the params of lm\n",
    "print('\\nCoefficcients:\\n', lm.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing R-Squared and coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squares = {'Mean Imputation': lm_mean.rsquared_adj, \n",
    "             'KNN Imputation': lm_KNN.rsquared_adj, \n",
    "             'MICE Imputation': lm_MICE.rsquared_adj}\n",
    "\n",
    "# Select best R-squared\n",
    "best_imputation = max(r_squares, key=r_squares.get)\n",
    "\n",
    "print(\"The best imputation technique is: \", best_imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Density plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graphs of imputed DataFrames and the complete case\n",
    "diabetes_cc['Skin_Fold'].plot(kind='kde', c='red', linewidth=3)\n",
    "diabetes_mean_imputed['Skin_Fold'].plot(kind='kde')\n",
    "diabetes_knn_imputed['Skin_Fold'].plot(kind='kde')\n",
    "diabetes_mice_imputed['Skin_Fold'].plot(kind='kde')\n",
    "\n",
    "# Create labels for the four DataFrames\n",
    "labels = ['Baseline (Complete Case)', 'Mean Imputation', 'KNN Imputation', 'MICE Imputation']\n",
    "plt.legend(labels)\n",
    "\n",
    "# Set the x-label as Skin Fold\n",
    "plt.xlabel('Skin Fold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
